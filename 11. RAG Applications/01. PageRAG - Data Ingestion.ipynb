{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG Concept - Data Ingestion Pipeline for PageRAG\n",
    "### Page-wise Document Processing with LLM Metadata Extraction\n",
    "\n",
    "Learning Objectives:\n",
    "- Extract text from PDFs page by page\n",
    "- Extract metadata using LLM (structured output)\n",
    "- Store in ChromaDB with rich metadata\n",
    "\n",
    "#### Real-World Use Cases:\n",
    "1. Financial Analysis: Process SEC filings (10-K, 10-Q)\n",
    "2. Legal: Organize contracts and case documents\n",
    "3. Research: Index academic papers with smart metadata\n",
    "4. Enterprise: Build searchable document repositories\n",
    "5. Compliance: Track regulatory documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced RAG - Data Ingestion Pipeline\n",
    "# Page-wise Document Processing with Filename-based Metadata Extraction\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from docling.document_converter import DocumentConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"data\"\n",
    "CHROMA_DIR = \"./chroma_financial_db\"\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "BASE_URL = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=CHROMA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from filename\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract metadata from filename.\n",
    "    \n",
    "    Expected format: {company} {doc_type} {quarter} {year}.pdf\n",
    "    Examples:\n",
    "    - amazon 10-k 2024.pdf\n",
    "    - amazon 10-q q2 2024.pdf\n",
    "    - apple 8-k q1 2024.pdf\n",
    "    \n",
    "    Returns:\n",
    "        dict with company_name, doc_type, fiscal_year, fiscal_quarter\n",
    "    \"\"\"\n",
    "    # Remove .pdf extension and split\n",
    "    name = filename.replace('.pdf', '')\n",
    "    parts = name.split()\n",
    "    \n",
    "    metadata = {}\n",
    "    \n",
    "    if len(parts) >= 3:\n",
    "        metadata['company_name'] = parts[0]  # amazon, apple, google\n",
    "        metadata['doc_type'] = parts[1]      # 10-k, 10-q, 8-k\n",
    "        \n",
    "        # Check if there's a quarter (for 10-q reports)\n",
    "        if len(parts) == 4:\n",
    "            # Format: company 10-q q2 2024\n",
    "            metadata['fiscal_quarter'] = parts[2]  # q1, q2, q3, q4\n",
    "            metadata['fiscal_year'] = int(parts[3])\n",
    "        else:\n",
    "            # Format: company 10-k 2024\n",
    "            metadata['fiscal_quarter'] = None\n",
    "            metadata['fiscal_year'] = int(parts[2])\n",
    "    else:\n",
    "        # Fallback for unexpected format\n",
    "        metadata['company_name'] = 'unknown'\n",
    "        metadata['doc_type'] = 'other'\n",
    "        metadata['fiscal_year'] = 1970\n",
    "        metadata['fiscal_quarter'] = None\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_pages(pdf_path):\n",
    "    \"\"\"Extract text from each page of PDF.\"\"\"\n",
    "\n",
    "    # source = \"finance/Amazon 10-Q Aug 2025.pdf\"  # document per local path or URL\n",
    "    converter = DocumentConverter()\n",
    "    result = converter.convert(pdf_path)\n",
    "\n",
    "    page_break = \"<!-- page break -->\"\n",
    "    markdown_text = result.document.export_to_markdown(page_break_placeholder=\"<!-- page break -->\")\n",
    "    pages = markdown_text.split(page_break)\n",
    "    return pages\n",
    "\n",
    "# pages = extract_pdf_pages(\"finance/Amazon 10-Q Aug 2025.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_file_hash(pdf_path):\n",
    "    \"\"\"Compute SHA-256 hash of file content.\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track processed files (deduplication)\n",
    "try:\n",
    "    existing_docs = vector_store.get(where={\"file_hash\": {\"$ne\": \"\"}}, include=[\"metadatas\"])\n",
    "    processed_hashes = set(m.get('file_hash') for m in existing_docs['metadatas'] if m.get('file_hash'))\n",
    "except:\n",
    "    processed_hashes = set()\n",
    "\n",
    "print(f\"Found {len(processed_hashes)} already processed files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Process a single PDF file page by page.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Check if already processed (deduplication)\n",
    "    file_hash = compute_file_hash(pdf_path)\n",
    "    if file_hash in processed_hashes:\n",
    "        print(f\"[SKIP] Already processed\")\n",
    "        return {\"status\": \"skipped\", \"reason\": \"duplicate\", \"file\": pdf_path.name}\n",
    "\n",
    "    try:\n",
    "        # Step 1: Extract pages from PDF\n",
    "        print(\"\\n[1/3] Extracting pages from PDF...\")\n",
    "        pages = extract_pdf_pages(pdf_path)\n",
    "        print(f\"[OK] Extracted {len(pages)} pages\")\n",
    "\n",
    "        # Step 2: Extract metadata from filename\n",
    "        print(\"\\n[2/3] Extracting metadata from filename...\")\n",
    "        file_metadata = extract_metadata_from_filename(pdf_path.name)\n",
    "        print(f\"[OK] Company: {file_metadata['company_name']}, Doc Type: {file_metadata['doc_type']}, Year: {file_metadata['fiscal_year']}\")\n",
    "\n",
    "        # Step 3: Process each page\n",
    "        print(\"\\n[3/3] Processing pages...\")\n",
    "        processed_pages = []\n",
    "\n",
    "        for page_num, page_text in enumerate(pages, start=1):\n",
    "            # Create metadata dict for this page\n",
    "            metadata_dict = file_metadata.copy()\n",
    "            metadata_dict['page'] = page_num\n",
    "            metadata_dict['file_hash'] = file_hash\n",
    "            metadata_dict['source_file'] = pdf_path.name\n",
    "\n",
    "            processed_pages.append({\n",
    "                'text': page_text,\n",
    "                'metadata': metadata_dict\n",
    "            })\n",
    "\n",
    "        # Step 4: Add to vector store\n",
    "        print(f\"\\n[4/4] Adding {len(processed_pages)} pages to vector store...\")\n",
    "        documents = [\n",
    "            Document(page_content=page['text'], metadata=page['metadata'])\n",
    "            for page in processed_pages\n",
    "        ]\n",
    "\n",
    "        vector_store.add_documents(documents=documents)\n",
    "\n",
    "        print(f\"[OK] Successfully ingested {len(processed_pages)} pages\")\n",
    "        processed_hashes.add(file_hash)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"file\": pdf_path.name,\n",
    "            \"pages\": len(processed_pages),\n",
    "            \"file_hash\": file_hash\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Processing {pdf_path.name}: {str(e)}\")\n",
    "        return {\"status\": \"error\", \"file\": pdf_path.name, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect PDF files from data directory (recursively)\n",
    "data_path = Path(DATA_DIR)\n",
    "pdf_files = list(data_path.rglob(\"*.pdf\"))  # rglob searches recursively\n",
    "print(f\"Found {len(pdf_files)} PDF files\")\n",
    "pdf_files[:5]  # Show first 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for pdf_path in pdf_files:\n",
    "    result = process_pdf(pdf_path)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count = sum(1 for r in results if r['status'] == 'success')\n",
    "skipped_count = sum(1 for r in results if r['status'] == 'skipped')\n",
    "error_count = sum(1 for r in results if r['status'] == 'error')\n",
    "\n",
    "print(f\"[OK] Successfully processed: {success_count}\")\n",
    "print(f\"[SKIP] Skipped (duplicates): {skipped_count}\")\n",
    "print(f\"[ERROR] Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(filter_dict={}, k=5):\n",
    "    \"\"\"\n",
    "    Create a retriever from the vector store with optional metadata filters.\n",
    "    \n",
    "    Args:\n",
    "        filter_dict: Dictionary of filters to apply\n",
    "                    Example: {\"company_name\": \"amazon\", \"doc_type\": \"10-q\"}\n",
    "        k: Number of results to retrieve (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        Retriever object\n",
    "    \n",
    "    Example:\n",
    "        # Single filter\n",
    "        retriever = create_retriever(filter_dict={\"company_name\": \"amazon\"})\n",
    "        \n",
    "        # Multiple filters\n",
    "        retriever = create_retriever(filter_dict={\n",
    "            \"company_name\": \"amazon\",\n",
    "            \"doc_type\": \"10-q\",\n",
    "            \"fiscal_year\": 2024\n",
    "        })\n",
    "        \n",
    "        # No filters\n",
    "        retriever = create_retriever()\n",
    "    \"\"\"\n",
    "    search_kwargs = {\"k\": k}\n",
    "    \n",
    "    if filter_dict:\n",
    "        # Convert flat dict to ChromaDB $and format if multiple conditions\n",
    "        if len(filter_dict) == 1:\n",
    "            # Single condition - use directly\n",
    "            search_kwargs[\"filter\"] = filter_dict\n",
    "        else:\n",
    "            # Multiple conditions - wrap in $and\n",
    "            filter_conditions = [{key: value} for key, value in filter_dict.items()]\n",
    "            search_kwargs[\"filter\"] = {\"$and\": filter_conditions}\n",
    "    \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter by company\n",
    "retriever = create_retriever(filter_dict={'fiscal_year': 2024, 'doc_type': '10-k', 'company_name': 'amazon'})\n",
    "\n",
    "result = retriever.invoke(\"revenue and financial results\")\n",
    "for i, doc in enumerate(result, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Company: {doc.metadata.get('company_name', 'N/A')}\")\n",
    "    print(f\"Doc Type: {doc.metadata.get('doc_type', 'N/A')}\")\n",
    "    print(f\"Year: {doc.metadata.get('fiscal_year', 'N/A')}\")\n",
    "    print(f\"Page: {doc.metadata.get('page', 'N/A')}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
