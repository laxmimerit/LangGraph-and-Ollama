{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe01b6d",
   "metadata": {},
   "source": [
    "## Self-RAG\n",
    "\n",
    "This notebook implements Self-RAG (Self-Reflective Retrieval-Augmented Generation), which combines:\n",
    "- Document relevance grading\n",
    "- Hallucination detection\n",
    "- Answer completeness checking\n",
    "- Query rewriting for better retrieval\n",
    "\n",
    "Flow:\n",
    "1. Query -> Retrieve -> Grade documents\n",
    "2. If relevant -> Generate -> Check hallucinations -> Check answer quality\n",
    "3. If not relevant -> Transform query -> Retrieve again\n",
    "4. If hallucinations -> Regenerate\n",
    "5. If doesn't answer query -> Transform query -> Retrieve again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b7267",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2310.11511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0606e707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "from typing import List\n",
    "import os\n",
    "import operator\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from scripts import my_tools\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb30de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "LLM_MODEL = \"qwen3\"\n",
    "BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "llm = ChatOllama(model=LLM_MODEL, base_url=BASE_URL, reasoning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84f2c46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! ðŸ˜Š How can I assist you today? If you have any questions or need help with something, feel free to let me know!', additional_kwargs={'reasoning_content': 'Okay, the user said \"hi /think\". I need to respond appropriately. First, I should acknowledge their greeting. Since they included \"/think\", maybe they\\'re testing if I can process that command. I should check if there\\'s a specific action needed when someone says \"hi /think\". Maybe they want me to think about something before responding. I should respond in a friendly manner, confirm that I received their message, and ask how I can assist them. Keep it open-ended so they can specify their needs. Make sure the response is natural and not robotic. Also, check for any possible misunderstandings. Alright, time to draft a response.\\n'}, response_metadata={'model': 'qwen3', 'created_at': '2025-11-16T06:32:33.7685606Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1547958700, 'load_duration': 64246400, 'prompt_eval_count': 11, 'prompt_eval_duration': 80169700, 'eval_count': 165, 'eval_duration': 1359909400, 'model_name': 'qwen3', 'model_provider': 'ollama'}, id='lc_run--66bf1ca8-9fae-4c10-a4ef-d51b562d85a5-0', usage_metadata={'input_tokens': 11, 'output_tokens': 165, 'total_tokens': 176})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e752476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Pydantic Schemas for Structured Outputs\n",
    "# =============================================================================\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the query, 'yes' or 'no'\")\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer\"\"\"\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses query\"\"\"\n",
    "    binary_score: str = Field(description=\"Answer addresses the query, 'yes' or 'no'\")\n",
    "\n",
    "class SearchQueries(BaseModel):\n",
    "    \"\"\"Search queries for retrieving missing information\"\"\"\n",
    "    search_queries: List[str] = Field(description=\"1-3 search queries to retrieve missing information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26229807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# State\n",
    "# =============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List, operator.add]\n",
    "    query: str\n",
    "    documents: str\n",
    "    filtered_documents: str\n",
    "    rewritten_queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c56703b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LangGraph Nodes\n",
    "# =============================================================================\n",
    "\n",
    "# Retrieve documents based on user query\n",
    "def retrieve_node(state:AgentState):\n",
    "    print(\"[RETRIEVE] Fetching documents\")\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    rewritten_queries = state.get(\"rewritten_queries\", [])\n",
    "\n",
    "    # Use rewritten queries if present, otherwise use original query\n",
    "    queries_to_search = rewritten_queries if rewritten_queries else [query]\n",
    "\n",
    "    all_results = []\n",
    "    for idx, search_query in enumerate(queries_to_search, 1):\n",
    "        print(f\"[RETRIEVE] {idx} Query: {search_query}\")\n",
    "        result = my_tools.retrieve_docs.invoke({'query': search_query, 'k': 3})\n",
    "        all_results.append(f\"## Query {idx}: {search_query}\\n\\n### Retrieved Documents:\\n{result}\")\n",
    "\n",
    "    combined_result = \"\\n\\n\" + \"\\n\\n\".join(all_results)\n",
    "    print(f\"[RETRIEVE] Documents fetched for {len(queries_to_search)} queries\")\n",
    "\n",
    "    # Save for debugging\n",
    "    os.makedirs(\"debug_logs\", exist_ok=True)\n",
    "    with open(\"debug_logs/self_rag_retrieved_docs.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Original Query: {query}\\n\")\n",
    "        if rewritten_queries:\n",
    "            f.write(f\"Rewritten Queries: {rewritten_queries}\\n\\n\")\n",
    "        f.write(combined_result)\n",
    "\n",
    "    return {\n",
    "        \"documents\": combined_result,\n",
    "        \"query\": query\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fda48069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade document relevance and filter out irrelevant ones\n",
    "def grade_documents_node(state:AgentState):\n",
    "    print(\"[GRADE] Evaluating document relevance\")\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    documents = state.get(\"documents\", \"\")\n",
    "\n",
    "    llm_structured = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "    system_prompt = \"\"\"You are a grader assessing relevance of retrieved documents to a user query.\n",
    "\n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the user query, grade it as relevant.\n",
    "\n",
    "Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the query.\"\"\"\n",
    "\n",
    "    system_msg = SystemMessage(system_prompt)\n",
    "    user_msg = HumanMessage(f\"Retrieved documents:\\n\\n{documents}\\n\\nUser query: {query}\")\n",
    "    \n",
    "    messages = [system_msg, user_msg]\n",
    "    response = llm_structured.invoke(messages)\n",
    "\n",
    "    print(f\"[GRADE] Relevance: {response.binary_score}\")\n",
    "\n",
    "    if response.binary_score == \"yes\":\n",
    "        return {\n",
    "            \"filtered_documents\": documents,\n",
    "            \"query\": query\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"filtered_documents\": \"\",\n",
    "            \"query\": query\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc292777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer based on retrieved documents\n",
    "def generate_node(state:AgentState):\n",
    "    print(\"[GENERATE] Creating answer\")\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    documents = state.get(\"filtered_documents\", \"\")\n",
    "\n",
    "    system_prompt = \"\"\"You are a financial document analyst providing detailed, accurate answers.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Write a comprehensive answer (200-300 words) in MARKDOWN format:\n",
    "- Use ## headings for sections\n",
    "- Use **bold** for emphasis\n",
    "- Use bullet points or numbered lists\n",
    "- Include inline citations like [1], [2] where applicable\n",
    "\n",
    "GUIDELINES:\n",
    "- Base your answer ONLY on the provided documents\n",
    "- Be specific with numbers, dates, and metrics\n",
    "- If information is missing, acknowledge it\n",
    "- Use proper financial terminology\n",
    "\n",
    "CITATIONS:\n",
    "At the end, list references in this format:\n",
    "**References:**\n",
    "1. Company: x, Year: y, Quarter: z, Page: n\"\"\"\n",
    "\n",
    "    query_prompt = f\"Documents:\\n\\n{documents}\\n\\nQuery: {query}\"\n",
    "\n",
    "    system_msg = SystemMessage(system_prompt)\n",
    "    user_msg = HumanMessage(query_prompt)\n",
    "    \n",
    "    messages = [system_msg, user_msg]\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    print(f\"[GENERATE] Answer created ({len(response.content)} chars)\")\n",
    "\n",
    "    # Save for debugging\n",
    "    os.makedirs(\"debug_logs\", exist_ok=True)\n",
    "    with open(\"debug_logs/self_rag_generation.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Query: {query}\\n\\n\")\n",
    "        f.write(response.content)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe024c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the query to produce better search queries\n",
    "def transform_query_node(state:AgentState):\n",
    "    print(\"[TRANSFORM] Rewriting query\")\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    rewritten_queries = state.get(\"rewritten_queries\", [])\n",
    "\n",
    "    llm_structured = llm.with_structured_output(SearchQueries)\n",
    "\n",
    "    system_prompt = \"\"\"You are a query re-writer that decomposes complex queries into focused search queries optimized for vectorstore retrieval.\n",
    "\n",
    "DECOMPOSITION STRATEGY:\n",
    "Break down the original query into 1-3 specific, focused queries where each query targets:\n",
    "- A single company (e.g., \"Amazon revenue 2023\" vs \"Google revenue 2023\")\n",
    "- A specific time period (e.g., \"Q1 2024\" vs \"Q2 2024\")\n",
    "- A specific metric or aspect (e.g., \"revenue\" vs \"net income\")\n",
    "- A specific document section (e.g., \"risk factors\" vs \"business overview\")\n",
    "\n",
    "GUIDELINES:\n",
    "- Expand abbreviations (e.g., \"rev\" -> \"revenue\", \"GOOGL\" -> \"Google\")\n",
    "- Add financial context if missing\n",
    "- Make each query self-contained and specific\n",
    "- Keep queries concise but clear (5-10 words each)\n",
    "- Avoid repeating previously tried queries\n",
    "\n",
    "EXAMPLES:\n",
    "- \"Compare Apple and Google revenue in 2024 Q1\" â†’ \n",
    "  [\"Apple total revenue Q1 2024\", \"Google total revenue Q1 2024\"]\n",
    "  \n",
    "- \"Amazon's revenue growth from 2022 to 2024\" â†’\n",
    "  [\"Amazon revenue 2022\", \"Amazon revenue 2023\", \"Amazon revenue 2024\"]\n",
    "  \n",
    "- \"What were the main risks for Microsoft in 2023?\" â†’\n",
    "  [\"Microsoft risk factors 2023\", \"Microsoft business challenges 2023\"]\"\"\"\n",
    "\n",
    "    query_context = f\"Original query: {query}\"\n",
    "    if rewritten_queries:\n",
    "        query_context += f\"\\n\\nPreviously tried queries:\\n\" + \"\\n\".join(f\"- {q}\" for q in rewritten_queries)\n",
    "    query_context += \"\\n\\nGenerate 1-3 focused search queries that decompose the original query. Each query should target a specific aspect.\"\n",
    "\n",
    "    system_msg = SystemMessage(system_prompt)\n",
    "    user_msg = HumanMessage(query_context)\n",
    "    \n",
    "    messages = [system_msg, user_msg]\n",
    "    response = llm_structured.invoke(messages)\n",
    "    \n",
    "    new_queries = response.search_queries\n",
    "\n",
    "    print(f\"[TRANSFORM] Original: {query}\")\n",
    "    print(f\"[TRANSFORM] Decomposed Queries: {new_queries}\")\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"rewritten_queries\": new_queries\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8eb5560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Router Logic\n",
    "# =============================================================================\n",
    "\n",
    "# Decide whether to generate answer or transform query\n",
    "def should_generate(state:AgentState):\n",
    "    print(\"[ROUTER] Assess graded documents\")\n",
    "\n",
    "    filtered_documents = state.get(\"filtered_documents\", \"\")\n",
    "\n",
    "    if not filtered_documents or filtered_documents.strip() == \"\":\n",
    "        print(\"[ROUTER] No relevant documents - transforming query\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"[ROUTER] Have relevant documents - generating answer\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6084b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for hallucinations and whether answer addresses query\n",
    "def check_answer_quality(state:AgentState):\n",
    "    print(\"[ROUTER] Check hallucinations and answer quality\")\n",
    "\n",
    "    query = state[\"query\"]\n",
    "    documents = state.get(\"filtered_documents\", \"\")\n",
    "    generation = state[\"messages\"][-1].content\n",
    "\n",
    "    # Step 1: Check hallucinations\n",
    "    print(\"[ROUTER] Checking hallucinations\")\n",
    "    llm_hallucination = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "    hallucination_prompt = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\n",
    "\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "    system_msg = SystemMessage(hallucination_prompt)\n",
    "    user_msg = HumanMessage(f\"Set of facts:\\n\\n{documents}\\n\\nLLM generation: {generation}\")\n",
    "    \n",
    "    messages = [system_msg, user_msg]\n",
    "    hallucination_response = llm_hallucination.invoke(messages)\n",
    "    \n",
    "    hallucination_grade = hallucination_response.binary_score\n",
    "\n",
    "    if hallucination_grade == \"yes\":\n",
    "        print(\"[ROUTER] Generation is grounded in documents\")\n",
    "\n",
    "        # Step 2: Check if answer addresses query\n",
    "        print(\"[ROUTER] Checking answer quality\")\n",
    "        llm_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "        answer_prompt = \"\"\"You are a grader assessing whether an answer addresses / resolves a query.\n",
    "\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the query.\"\"\"\n",
    "\n",
    "        system_msg = SystemMessage(answer_prompt)\n",
    "        user_msg = HumanMessage(f\"User query:\\n\\n{query}\\n\\nLLM generation: {generation}\")\n",
    "        \n",
    "        messages = [system_msg, user_msg]\n",
    "        answer_response = llm_answer.invoke(messages)\n",
    "        \n",
    "        answer_grade = answer_response.binary_score\n",
    "\n",
    "        if answer_grade == \"yes\":\n",
    "            print(\"[ROUTER] Generation addresses query - USEFUL\")\n",
    "            return END\n",
    "        else:\n",
    "            print(\"[ROUTER] Generation does NOT address query - NOT USEFUL\")\n",
    "            return \"transform_query\"\n",
    "    else:\n",
    "        print(\"[ROUTER] Generation NOT grounded in documents - NOT SUPPORTED\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "914a1027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000001B3D52203B0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build Graph\n",
    "# =============================================================================\n",
    "\n",
    "def create_self_rag():\n",
    "\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # add nodes\n",
    "    builder.add_node('retrieve', retrieve_node)\n",
    "    builder.add_node('grade_documents', grade_documents_node)\n",
    "    builder.add_node('generate', generate_node)\n",
    "    builder.add_node('transform_query', transform_query_node)\n",
    "\n",
    "    # define edges\n",
    "    builder.add_edge(START, 'retrieve')\n",
    "    builder.add_edge('retrieve', 'grade_documents')\n",
    "    builder.add_edge('transform_query', 'retrieve')\n",
    "\n",
    "    # conditional edges\n",
    "    builder.add_conditional_edges('grade_documents', should_generate, ['transform_query', 'generate'])\n",
    "    builder.add_conditional_edges('generate', check_answer_quality, ['generate', END, 'transform_query'])\n",
    "\n",
    "    return builder.compile()\n",
    "\n",
    "graph = create_self_rag()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a992269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRIEVE] Fetching documents\n",
      "[RETRIEVE] 1 Query: What was Amazon's revenue in 2023?\n",
      "\n",
      "[TOOL] retrieve_docs called\n",
      "[QUERY] What was Amazon's revenue in 2023?\n",
      "   [1] Doc 18: score=23.4068\n",
      "   [2] Doc 2: score=22.7389\n",
      "   [3] Doc 10: score=20.1882\n",
      "[RETRIEVED] 3 documents\n",
      "[RETRIEVE] Documents fetched for 1 queries\n",
      "[GRADE] Evaluating document relevance\n",
      "[GRADE] Relevance: yes\n",
      "[ROUTER] Assess graded documents\n",
      "[ROUTER] Have relevant documents - generating answer\n",
      "[GENERATE] Creating answer\n",
      "[GENERATE] Answer created (1687 chars)\n",
      "[ROUTER] Check hallucinations and answer quality\n",
      "[ROUTER] Checking hallucinations\n",
      "[ROUTER] Generation is grounded in documents\n",
      "[ROUTER] Checking answer quality\n",
      "[ROUTER] Generation addresses query - USEFUL\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Demo\n",
    "# =============================================================================\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(\"What was Amazon's revenue in 2023?\")],\n",
    "    \"query\": \"What was Amazon's revenue in 2023?\",\n",
    "    \"rewritten_queries\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de0588da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "## Amazon's 2023 Revenue\n",
      "\n",
      "Amazon's total **revenue (net sales)** for the fiscal year ended December 31, 2023, was **$574.785 billion** [2]. This represents a **12% year-over-year increase** compared to 2022, when revenue was $513.983 billion [2]. \n",
      "\n",
      "### Breakdown by Segment\n",
      "Amazon's revenue is divided into three segments:\n",
      "- **North America**: $352.828 billion (61% of total revenue)\n",
      "- **International**: $131.200 billion (23% of total revenue)\n",
      "- **AWS**: $90.757 billion (16% of total revenue)\n",
      "\n",
      "### Key Drivers of Growth\n",
      "- **North America** saw 12% growth, driven by increased unit sales from third-party sellers, advertising, and subscription services [2].\n",
      "- **International** revenue rose 11%, benefiting from similar factors, with foreign exchange rates positively impacting results by $88 million [2].\n",
      "- **AWS** grew by 13%, contributing significantly to overall expansion [2].\n",
      "\n",
      "### Adjustments and Context\n",
      "- **Foreign exchange rates** reduced net sales by $71 million in 2023, though the 12% growth rate reflects adjustments for these effects [2].\n",
      "- The **consolidated revenue mix** remained consistent across segments, with no significant shifts in proportion [2].\n",
      "\n",
      "### Additional Insights\n",
      "- **Unearned revenue** as of December 31, 2023, reached $20.6 billion, reflecting prepayments for AWS services and Amazon Prime memberships [3].\n",
      "- **Free cash flow** remained a key focus, though specific figures are detailed in the \"Non-GAAP Financial Measures\" section of the 10-K [1].\n",
      "\n",
      "**References:**\n",
      "1. Company: Amazon, Year: 2023, Quarter: Full Year, Page: 20  \n",
      "2. Company: Amazon, Year: 2023, Quarter: Full Year, Page: 24  \n",
      "3. Company: Amazon, Year: 2023, Quarter: Full Year, Page: 51\n"
     ]
    }
   ],
   "source": [
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRIEVE] Fetching documents\n",
      "[RETRIEVE] 1 Query: Compare Apple and Amazon revenue in 2024 q1\n",
      "\n",
      "[TOOL] retrieve_docs called\n",
      "[QUERY] Compare Apple and Amazon revenue in 2024 q1\n",
      "   [1] Doc 1: score=21.6328\n",
      "   [2] Doc 3: score=20.5197\n",
      "   [3] Doc 0: score=17.8864\n",
      "[RETRIEVED] 3 documents\n",
      "[RETRIEVE] Documents fetched for 1 queries\n",
      "[GRADE] Evaluating document relevance\n",
      "[GRADE] Relevance: no\n",
      "[ROUTER] Assess graded documents\n",
      "[ROUTER] No relevant documents - transforming query\n",
      "[TRANSFORM] Rewriting query\n",
      "[TRANSFORM] Original: Compare Apple and Amazon revenue in 2024 q1\n",
      "[TRANSFORM] Decomposed Queries: ['Apple total revenue Q1 2024', 'Amazon total revenue Q1 2024']\n",
      "[RETRIEVE] Fetching documents\n",
      "[RETRIEVE] 1 Query: Apple total revenue Q1 2024\n",
      "\n",
      "[TOOL] retrieve_docs called\n",
      "[QUERY] Apple total revenue Q1 2024\n",
      "   [1] Doc 1: score=21.6328\n",
      "   [2] Doc 3: score=20.5197\n",
      "   [3] Doc 0: score=17.8864\n",
      "[RETRIEVED] 3 documents\n",
      "[RETRIEVE] 2 Query: Amazon total revenue Q1 2024\n",
      "\n",
      "[TOOL] retrieve_docs called\n",
      "[QUERY] Amazon total revenue Q1 2024\n"
     ]
    }
   ],
   "source": [
    "query = \"Compare Apple and Amazon revenue in 2024 q1\"\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(query)],\n",
    "    \"query\": query,\n",
    "    \"rewritten_queries\": []\n",
    "})\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697468b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RETRIEVE] Fetching documents\n",
      "[RETRIEVE] 1 Query: What was Google's revenue in 2023?\n",
      "\n",
      "[TOOL] retrieve_docs called\n",
      "[QUERY] What was Google's revenue in 2023?\n",
      "   [1] Doc 21: score=25.8314\n",
      "   [2] Doc 5: score=23.4856\n",
      "   [3] Doc 16: score=21.7380\n",
      "[RETRIEVED] 3 documents\n",
      "[RETRIEVE] Documents fetched for 1 queries\n",
      "[GRADE] Evaluating document relevance\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat was Google\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms revenue in 2023?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrewritten_queries\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m result[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].pretty_print()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langgraph\\pregel\\main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langgraph\\pregel\\main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgrade_documents_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     19\u001b[39m user_msg = HumanMessage(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrieved documents:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdocuments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUser query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m messages = [system_msg, user_msg]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m response = \u001b[43mllm_structured\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[GRADE] Relevance: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.binary_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.binary_score == \u001b[33m\"\u001b[39m\u001b[33myes\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3091\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3089\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3090\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3091\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3092\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3093\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5492\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5485\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5487\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5490\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5491\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5495\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:379\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m     **kwargs: Any,\n\u001b[32m    373\u001b[39m ) -> AIMessage:\n\u001b[32m    374\u001b[39m     config = ensure_config(config)\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    376\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m         cast(\n\u001b[32m    378\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    389\u001b[39m         ).message,\n\u001b[32m    390\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1088\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1081\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m     **kwargs: Any,\n\u001b[32m   1086\u001b[39m ) -> LLMResult:\n\u001b[32m   1087\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:903\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    902\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m         )\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    911\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1192\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1190\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1025\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1019\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1020\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1029\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1030\u001b[39m         message=AIMessage(\n\u001b[32m   1031\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1038\u001b[39m         generation_info=generation_info,\n\u001b[32m   1039\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:960\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    952\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    953\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    957\u001b[39m     **kwargs: Any,\n\u001b[32m    958\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    959\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1049\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1043\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1044\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1045\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1046\u001b[39m     **kwargs: Any,\n\u001b[32m   1047\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1048\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1055\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\langchain_ollama\\chat_models.py:947\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    946\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    949\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\ollama\\_client.py:181\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m    179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpx\\_models.py:929\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m decoder = LineDecoder()\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpx\\_models.py:916\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    914\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpx\\_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpx\\_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpx\\_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpx\\_transports\\default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_sync\\http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_sync\\http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_sync\\http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laxmi\\anaconda3\\envs\\ml\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "query = \"What was Google's revenue in 2023?\"\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"messages\": [HumanMessage(query)],\n",
    "    \"query\": query,\n",
    "    \"rewritten_queries\": []\n",
    "})\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad20fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059821dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
