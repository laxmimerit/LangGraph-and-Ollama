{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dc6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "from typing import Dict, List\n",
    "import os\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "import operator\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from scripts import my_tools\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "LLM_MODEL = \"qwen3\"\n",
    "# LLM_MODEL = \"gpt-oss\"\n",
    "MAX_ITERATIONS = 3\n",
    "llm = ChatOllama(model=LLM_MODEL, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4ebfc",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Pydantic Schemas for Structured Output\n# =============================================================================\n\nclass Reflection(BaseModel):\n    \"\"\"Critique of current answer.\"\"\"\n    missing: str = Field(description=\"What critical information is missing or incomplete\")\n    superfluous: str = Field(description=\"What information is unnecessary or redundant\")\n\nclass Answer(BaseModel):\n    \"\"\"Answer with inline citations, reflection, and search queries.\"\"\"\n    answer: str = Field(\n        description=\"Detailed answer (~250-300 words) with inline citations [1], [2] and reference list at the end\"\n    )\n    reflection: Reflection = Field(description=\"Critical reflection on the answer\")\n    search_queries: List[str] = Field(\n        default_factory=list,\n        description=\"1-3 search queries if more information needed, empty if complete\"\n    )\n    is_complete: bool = Field(\n        default=False,\n        description=\"True if answer is complete and no more searches needed\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# State\n",
    "# =============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State for reflexion RAG system.\"\"\"\n",
    "    messages: Annotated[List, add_messages]\n",
    "    iteration_count: int\n",
    "    retrieved_docs: str\n",
    "    search_queries: List[str]\n",
    "    is_complete: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947bb74",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# LangGraph Nodes\n# =============================================================================\n\ndef draft_node(state: AgentState) -> Dict:\n    \"\"\"Generate initial answer with reflection.\"\"\"\n    print(\"\\n[NODE] Draft - Generating initial answer\")\n    \n    llm_structured = llm.with_structured_output(Answer)\n    \n    system_prompt = \"\"\"You are an expert financial document researcher.\n\nTASK:\n1. Provide detailed answer (~250 words) to user's question\n2. Use Markdown formatting (headings, bullets, tables, bold)\n3. Reflect critically: identify missing and superfluous information\n4. Generate 1-3 specific search queries to retrieve missing information\n\nFormat your answer with inline citations [1], [2] if you have any prior knowledge to cite, otherwise note what information you need.\n\nOutput JSON matching this schema:\n{\n  \"answer\": \"Your detailed answer with inline citations [1] and references at the end...\",\n  \"reflection\": {\n    \"missing\": \"What information is missing...\",\n    \"superfluous\": \"What is unnecessary...\"\n  },\n  \"search_queries\": [\"specific query 1\", \"specific query 2\"],\n  \"is_complete\": false\n}\"\"\"\n    \n    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n    response = llm_structured.invoke(messages)\n    \n    print(f\"[DRAFT] Generated answer with {len(response.search_queries)} search queries\")\n    print(f\"[REFLECTION] Missing: {response.reflection.missing[:100]}...\")\n    \n    ai_message = AIMessage(\n        content=f\"**Answer:**\\n\\n{response.answer}\\n\\n\"\n                f\"**Reflection:**\\n- Missing: {response.reflection.missing}\\n\"\n                f\"- Superfluous: {response.reflection.superfluous}\\n\\n\"\n                f\"**Search Queries:** {', '.join(response.search_queries)}\"\n    )\n    \n    return {\n        \"messages\": [ai_message],\n        \"iteration_count\": 1,\n        \"search_queries\": response.search_queries\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: AgentState) -> Dict:\n",
    "    \"\"\"Execute search queries and retrieve documents.\"\"\"\n",
    "    print(\"\\n[NODE] Retrieve - Fetching documents\")\n",
    "    \n",
    "    # Get search queries from the last state update\n",
    "    search_queries = state.get(\"search_queries\", [])\n",
    "    \n",
    "    if not search_queries:\n",
    "        print(\"[RETRIEVE] No search queries found, skipping retrieval\")\n",
    "        return {\"retrieved_docs\": \"\"}\n",
    "    \n",
    "    # Process each query using my_tools.retrieve_docs\n",
    "    print(f\"\\n[RETRIEVAL] Processing {len(search_queries)} queries\")\n",
    "    \n",
    "    all_retrieved_text = []\n",
    "    \n",
    "    for query_idx, query in enumerate(search_queries, 1):\n",
    "        print(f\"[QUERY {query_idx}] {query}\")\n",
    "        \n",
    "        # Call my_tools.retrieve_docs for each query\n",
    "        result = my_tools.retrieve_docs.invoke({'query': query, 'k': 3})\n",
    "        \n",
    "        # Format with query header\n",
    "        all_retrieved_text.append(f\"\\n--- Query {query_idx}: {query} ---\\n{result}\")\n",
    "    \n",
    "    # Combine all results\n",
    "    combined_result = \"\\n\\n\".join(all_retrieved_text)\n",
    "    \n",
    "    # Save for debugging\n",
    "    os.makedirs(\"debug_logs\", exist_ok=True)\n",
    "    with open(\"debug_logs/reflexion_retrieved_docs.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(combined_result)    \n",
    "    \n",
    "    return {\n",
    "        \"retrieved_docs\": combined_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dd61f",
   "metadata": {},
   "outputs": [],
   "source": "\ndef revise_node(state: AgentState) -> Dict:\n    \"\"\"Revise answer with new information.\"\"\"\n    print(f\"\\n[NODE] Revise - Iteration {state.get('iteration_count', 1)}\")\n    \n    llm_structured = llm.with_structured_output(Answer)\n    retrieved_docs = state.get('retrieved_docs', 'No documents retrieved yet')\n    \n    system_prompt = \"\"\"You are an expert financial document researcher.\n\nTASK:\n1. Write DETAILED answer (~250-300 words) with MARKDOWN formatting (## headings, **bold**, bullets, tables)\n2. Include inline citations [1], [2] in the answer text\n3. Add reference list at the end: \"[1] Company: x, Year: y, Quarter: z, Page: n\"\n4. Critically reflect on what's missing or superfluous\n5. Generate 2-3 SPECIFIC search queries if information is incomplete\n\nCRITICAL - Search Query Rules:\n- Use specific keywords: company name, year, quarter, metric type\n- Good: \"Amazon q1 q2 2023 quarterly revenue\", \"Amazon 2023 AWS segment earnings\"\n- Bad: \"Amazon revenue\", \"financial data\"\n\nDECISION LOGIC:\nAsk yourself:\n- Do I have complete quarterly breakdown? (Q1, Q2, Q3, Q4)\n- Do I have segment-wise data if relevant?\n- Do I have year-over-year comparisons?\n- Do I have all metrics requested (revenue, income, margins, etc.)?\n- Do I have all companies mentioned in the question?\n\nIf ALL needed information present:\n{\n  \"answer\": \"## Complete answer with citations [1][2]...\\n\\n**References:**\\n[1] Company: amazon, Year: 2023, Page: 5\",\n  \"is_complete\": true,\n  \"search_queries\": [],\n  \"reflection\": {\"missing\": \"None\", \"superfluous\": \"None\"}\n}\n\nIf MISSING any required data:\n{\n  \"answer\": \"## Answer with available data [1]...\\n\\n**References:**\\n[1] Company: amazon, Year: 2023, Page: 5\",\n  \"is_complete\": false,\n  \"search_queries\": [\"specific query 1\", \"specific query 2\"],\n  \"reflection\": {\"missing\": \"List exact gaps\", \"superfluous\": \"None\"}\n}\n\nMANDATORY: If is_complete=false, you MUST provide 2-3 specific search_queries.\"\"\"\n\n    messages = [SystemMessage(content=system_prompt)] + \\\n                [state[\"messages\"][-1]] + \\\n                [HumanMessage(content=f\"RETRIEVED DOCUMENTS:\\n{retrieved_docs}\\n\\nRevise your answer using these documents. Output JSON only.\")]\n\n    response = llm_structured.invoke(messages)\n    \n    # Safety check\n    if not response.is_complete and not response.search_queries:\n        print(\"[WARNING] No search queries provided but answer incomplete. Forcing completion.\")\n        response.is_complete = True\n    \n    print(f\"[REVISE] Complete: {response.is_complete}\")\n    print(f\"[REVISE] New queries: {len(response.search_queries)}\")\n    if response.search_queries:\n        for i, q in enumerate(response.search_queries, 1):\n            print(f\"  => [{i}] {q}\")\n    \n    ai_message = AIMessage(\n        content=f\"**Answer:**\\n\\n{response.answer}\\n\\n\"\n                f\"**Reflection:**\\n- Missing: {response.reflection.missing}\\n\"\n                f\"- Superfluous: {response.reflection.superfluous}\\n\\n\"\n                f\"**Status:** {'Complete âœ“' if response.is_complete else 'Needs more information'}\"\n    )\n    \n    return {\n        \"messages\": [ai_message],\n        \"iteration_count\": state.get(\"iteration_count\", 1) + 1,\n        \"search_queries\": response.search_queries,\n        \"is_complete\": response.is_complete\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce33da1",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Router Logic\n# =============================================================================\n\ndef should_continue(state: AgentState) -> str:\n    \"\"\"Decide whether to continue reflection loop or end.\"\"\"\n    iteration_count = state.get(\"iteration_count\", 0)\n    is_complete = state.get(\"is_complete\", False)\n    search_queries = state.get(\"search_queries\", [])\n    \n    if is_complete or not search_queries or iteration_count >= MAX_ITERATIONS:\n        reason = \"complete\" if is_complete else \"no queries\" if not search_queries else f\"max iterations ({MAX_ITERATIONS})\"\n        print(f\"[ROUTER] Ending - {reason}\")\n        return END\n    \n    print(f\"[ROUTER] Iteration {iteration_count} - continuing to retrieve\")\n    return \"retrieve\"\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70580d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Build Graph\n",
    "# =============================================================================\n",
    "\n",
    "def create_reflexion_rag():\n",
    "    \"\"\"Build reflexion RAG graph.\"\"\"\n",
    "    print(\"\\n[GRAPH] Building reflexion RAG workflow\")\n",
    "    \n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Add nodes\n",
    "    builder.add_node(\"draft\", draft_node)\n",
    "    builder.add_node(\"retrieve\", retrieve_node)\n",
    "    builder.add_node(\"revise\", revise_node)\n",
    "\n",
    "    # Define edges\n",
    "    builder.add_edge(START, \"draft\")\n",
    "    builder.add_edge(\"draft\", \"retrieve\")\n",
    "    builder.add_edge(\"retrieve\", \"revise\")\n",
    "    builder.add_conditional_edges(\"revise\", should_continue, [\"retrieve\", END])\n",
    "\n",
    "    return builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ccda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_reflexion_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [HumanMessage(content=\"What was Amazon's balance sheet in 2023 fiscal year?\")],\n",
    "        \"iteration_count\": 0,\n",
    "        \"retrieved_docs\": \"\",\n",
    "        \"search_queries\": [],\n",
    "        \"is_complete\": False\n",
    "    })\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe9466",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [HumanMessage(content=\"Compare the Amazon's and Apple's revenue of 2024 Q1?\")],\n",
    "        \"iteration_count\": 0,\n",
    "        \"retrieved_docs\": \"\",\n",
    "        \"search_queries\": [],\n",
    "        \"is_complete\": False\n",
    "    })\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a604eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\n",
    "        \"messages\": [HumanMessage(content=\"Show segment wise earning for Iphones and Macbooks in 2023\")],\n",
    "        \"iteration_count\": 0,\n",
    "        \"retrieved_docs\": \"\",\n",
    "        \"search_queries\": [],\n",
    "        \"is_complete\": False\n",
    "    })\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8092249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}