{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b151a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from scripts.schemas import ChunkMetadata, RankingKeywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bc4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# ChromaDB Configuration (from PageRAG - Data Ingestion)\n",
    "CHROMA_DIR = \"chroma_financial_db\"\n",
    "COLLECTION_NAME = \"financial_docs\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "LLM_MODEL = \"qwen3\"\n",
    "BASE_URL = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debc651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "        model=LLM_MODEL,\n",
    "        base_url=BASE_URL\n",
    "    )\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    base_url=BASE_URL\n",
    ")\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=CHROMA_DIR\n",
    ")\n",
    "\n",
    "\n",
    "total_count = vector_store._collection.count()\n",
    "print(f\"[DB] Total documents in database: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda65a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filters(user_query: str):\n",
    "    \"\"\"Extract metadata filters from user query.\"\"\"\n",
    "    \n",
    "    llm_structured = llm.with_structured_output(ChunkMetadata)\n",
    "    \n",
    "    prompt = f\"\"\"Extract metadata filters from the query. Return None for fields not mentioned.\n",
    "\n",
    "                USER QUERY: {user_query}\n",
    "\n",
    "                COMPANY MAPPINGS:\n",
    "                - Amazon/AMZN -> amazon\n",
    "                - Google/Alphabet/GOOGL/GOOG -> google\n",
    "                - Apple/AAPL -> apple\n",
    "                - Microsoft/MSFT -> microsoft\n",
    "                - Tesla/TSLA -> tesla\n",
    "                - Nvidia/NVDA -> nvidia\n",
    "                - Meta/Facebook/FB -> meta\n",
    "\n",
    "                DOC TYPE:\n",
    "                - Annual report -> 10-k\n",
    "                - Quarterly report -> 10-q\n",
    "                - Current report -> 8-k\n",
    "\n",
    "                EXAMPLES:\n",
    "                \"Amazon Q3 2024 revenue\" -> {{\"company_name\": \"amazon\", \"doc_type\": \"10-q\", \"fiscal_year\": 2024, \"fiscal_quarter\": \"q3\"}}\n",
    "                \"Apple 2023 annual report\" -> {{\"company_name\": \"apple\", \"doc_type\": \"10-k\", \"fiscal_year\": 2023}}\n",
    "                \"Tesla profitability\" -> {{}}\n",
    "\n",
    "                Extract metadata:\n",
    "                \"\"\"\n",
    "    \n",
    "    metadata = llm_structured.invoke(prompt)\n",
    "    filters = metadata.model_dump(exclude_none=True)\n",
    "    \n",
    "    return filters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_ranking_keywords(user_query: str):\n",
    "    \"\"\"Generate EXACTLY 5 financial keywords for document ranking.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Generate EXACTLY 5 financial keywords from SEC filings terminology.\n",
    "\n",
    "                USER QUERY: {user_query}\n",
    "\n",
    "                USE EXACT TERMS FROM 10-K/10-Q FILINGS:\n",
    "\n",
    "                STATEMENT HEADINGS:\n",
    "                \"consolidated statements of operations\", \"consolidated balance sheets\", \"consolidated statements of cash flows\", \"consolidated statements of stockholders equity\"\n",
    "\n",
    "                INCOME STATEMENT:\n",
    "                \"revenue\", \"net revenue\", \"cost of revenue\", \"gross profit\", \"operating income\", \"net income\", \"earnings per share\"\n",
    "\n",
    "                BALANCE SHEET:\n",
    "                \"total assets\", \"cash and cash equivalents\", \"total liabilities\", \"stockholders equity\", \"working capital\", \"long-term debt\"\n",
    "\n",
    "                CASH FLOWS:\n",
    "                \"cash flows from operating activities\", \"net cash provided by operating activities\", \"cash flows from investing activities\", \"free cash flow\", \"capital expenditures\"\n",
    "\n",
    "                RULES:\n",
    "                - Return EXACTLY 5 keywords\n",
    "                - Use exact phrases from SEC filings\n",
    "                - Match query topic (revenue -> revenue terms, cash -> cash flow terms)\n",
    "                - Use \"cash flows\" (plural), \"stockholders equity\"\n",
    "\n",
    "                EXAMPLES:\n",
    "                \"revenue analysis\" -> [\"revenue\", \"net revenue\", \"total revenue\", \"consolidated statements of operations\", \"net sales\"]\n",
    "                \"cash flow performance\" -> [\"consolidated statements of cash flows\", \"cash flows from operating activities\", \"net cash provided by operating activities\", \"free cash flow\", \"operating activities\"]\n",
    "                \"balance sheet strength\" -> [\"consolidated balance sheets\", \"total assets\", \"stockholders equity\", \"cash and cash equivalents\", \"long-term debt\"]\n",
    "\n",
    "                Generate EXACTLY 5 keywords:\n",
    "                \"\"\"\n",
    "    \n",
    "    llm_structured = llm.with_structured_output(RankingKeywords)\n",
    "    result = llm_structured.invoke(prompt)\n",
    "    return result.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a842a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_search_kwargs(filters, ranking_keywords, k=3):\n",
    "    \"\"\"\n",
    "    Build search kwargs for ChromaDB retriever with proper filter formatting.\n",
    "\n",
    "    Handles the conversion of flat filter dictionaries to ChromaDB's $and format\n",
    "    when multiple filters are present, and adds where_document filtering for ranking keywords.\n",
    "\n",
    "    Args:\n",
    "        filters: Dictionary of metadata filters\n",
    "        ranking_keywords: List of keywords to search in document content\n",
    "        k: Number of results to retrieve\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with search_kwargs formatted for ChromaDB\n",
    "\n",
    "    Examples:\n",
    "        >>> build_search_kwargs({\"company_ticker\": \"AMZN\"}, [\"cash flow\", \"revenue\"], k=5)\n",
    "        {\n",
    "            \"k\": 5, \n",
    "            \"filter\": {\"company_ticker\": \"AMZN\"},\n",
    "            \"where_document\": {\"$or\": [{\"$contains\": \"cash flow\"}, {\"$contains\": \"revenue\"}]}\n",
    "        }\n",
    "\n",
    "        >>> build_search_kwargs({\"company_ticker\": \"AMZN\", \"fiscal_year\": 2023}, [\"liquidity\"], k=5)\n",
    "        {\n",
    "            \"k\": 5,\n",
    "            \"filter\": {\"$and\": [{\"company_ticker\": \"AMZN\"}, {\"fiscal_year\": 2023}]},\n",
    "            \"where_document\": {\"$or\": [{\"$contains\": \"liquidity\"}]}\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    # Build search kwargs\n",
    "    search_kwargs = {\"k\": k, 'fetch_k': k * 20}  # fetch_k for reranking\n",
    "\n",
    "    # Add metadata filters\n",
    "    if filters:\n",
    "        # Single filter: use directly\n",
    "        if len(filters) == 1:\n",
    "            search_kwargs[\"filter\"] = filters\n",
    "        # Multiple filters: combine with $and\n",
    "        else:\n",
    "            filter_conditions = [{key: value} for key, value in filters.items()]\n",
    "            search_kwargs[\"filter\"] = {\"$and\": filter_conditions}\n",
    "\n",
    "    # Add document content filters using ranking keywords\n",
    "    if ranking_keywords:\n",
    "        # Single keyword\n",
    "        if len(ranking_keywords) == 1:\n",
    "            search_kwargs[\"where_document\"] = {\"$contains\": ranking_keywords[0]}\n",
    "        # Multiple keywords: combine with $or (match ANY keyword)\n",
    "        else:\n",
    "            search_kwargs[\"where_document\"] = {\n",
    "                \"$or\": [\n",
    "                    {\"$contains\": keyword}\n",
    "                    for keyword in ranking_keywords\n",
    "                ]\n",
    "            }\n",
    "\n",
    "    return search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fae104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_docs(query, filters={}, ranking_keywords=[], k=5):\n",
    "    \"\"\"\n",
    "    Search documents with metadata and content filters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query text\n",
    "        filters (dict): Metadata filters (e.g., {\"company_name\": \"amazon\", \"fiscal_year\": 2023})\n",
    "        ranking_keywords (list): Keywords for content filtering (documents must contain at least one)\n",
    "        k (int): Number of results (default: 5)\n",
    "    \n",
    "    Returns:\n",
    "        list: Matching Document objects\n",
    "    \n",
    "    Example:\n",
    "        docs = search_docs(\n",
    "            query=\"Analyze cash flow\",\n",
    "            filters={\"company_name\": \"amazon\", \"doc_type\": \"10-k\"},\n",
    "            ranking_keywords=[\"cash flow\", \"liquidity\"],\n",
    "            k=10\n",
    "        )\n",
    "    \"\"\"\n",
    "    search_kwargs = build_search_kwargs(filters, ranking_keywords, k=k)\n",
    "    \n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    \n",
    "    return retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Show me Amazon's cash flow in 2023\"\n",
    "filters = extract_filters(query)\n",
    "filters\n",
    "\n",
    "ranking_keywords = generate_ranking_keywords(query)\n",
    "ranking_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d582445",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search_docs(query, filters, ranking_keywords, k=20)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Show me Amazon's revenue in 2023\"\n",
    "filters = extract_filters(query)\n",
    "ranking_keywords = generate_ranking_keywords(query)\n",
    "results = search_docs(query, filters, ranking_keywords, k=20)\n",
    "print(filters, ranking_keywords)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13890e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Plus\n",
    "\n",
    "def extract_headings_with_content(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract markdown headings with one paragraph of content after them.\n",
    "    \n",
    "    Args:\n",
    "        text: Document text content\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted heading + content chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Split by double newlines to get sections\n",
    "    sections = text.split('\\n\\n')\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(sections):\n",
    "        section = sections[i].strip()\n",
    "        \n",
    "        # Check if section starts with markdown heading (one or more #)\n",
    "        if re.match(r'^#+\\s+', section):\n",
    "            # Found a heading\n",
    "            heading = section\n",
    "            \n",
    "            # Get the next paragraph/content after heading\n",
    "            if i + 1 < len(sections):\n",
    "                next_content = sections[i + 1].strip()\n",
    "                chunk = f\"{heading}\\n\\n{next_content}\"\n",
    "                i += 2  # Skip both heading and next content\n",
    "            else:\n",
    "                chunk = heading\n",
    "                i += 1  # Only skip heading\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def rank_documents_by_keywords(docs, keywords, k=5):\n",
    "    \"\"\"\n",
    "    Rank documents using BM25Plus on heading+content chunks, then return full ranked documents.\n",
    "\n",
    "    Process:\n",
    "    1. Extract headings with one paragraph after them from each document\n",
    "    2. Rank these chunks using BM25Plus with keywords\n",
    "    3. Return documents sorted by their best chunk score\n",
    "\n",
    "    Args:\n",
    "        docs: List of Document objects to rank\n",
    "        keywords: List of keywords to rank by (e.g., ['consolidated balance sheets', 'total assets'])\n",
    "\n",
    "    Returns:\n",
    "        List of Document objects sorted by BM25 score (highest first)\n",
    "\n",
    "    Example:\n",
    "        >>> docs = retriever.invoke(\"Amazon balance sheet\")\n",
    "        >>> ranked = rank_documents_by_keywords(docs, ['consolidated balance sheets', 'total assets'])\n",
    "    \"\"\"\n",
    "    if not docs or not keywords:\n",
    "        return docs\n",
    "\n",
    "    # Tokenize keywords (query terms)\n",
    "    query_tokens = ' '.join(keywords).lower().split()\n",
    "\n",
    "    # Extract headings+content and track which doc they came from\n",
    "    doc_chunks = []  # List of (doc_index, chunk_text)\n",
    "    doc_best_scores = {}  # {doc_index: best_score}\n",
    "    \n",
    "    for doc_idx, doc in enumerate(docs):\n",
    "        # Extract heading+content chunks from document\n",
    "        chunks = extract_headings_with_content(doc.page_content)\n",
    "        \n",
    "        if not chunks:\n",
    "            # If no headings found, use full content\n",
    "            chunks = [doc.page_content]\n",
    "        \n",
    "        # Store chunks with their document index\n",
    "        for chunk in chunks:\n",
    "            doc_chunks.append((doc_idx, chunk))\n",
    "    \n",
    "    if not doc_chunks:\n",
    "        print(\"[BM25] No chunks extracted\")\n",
    "        return docs\n",
    "    \n",
    "    # Tokenize all chunks\n",
    "    tokenized_chunks = [chunk.lower().split() for _, chunk in doc_chunks]\n",
    "    \n",
    "    # Initialize BM25Plus on chunks\n",
    "    bm25 = BM25Plus(tokenized_chunks)\n",
    "    \n",
    "    # Get BM25 scores for all chunks\n",
    "    chunk_scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Find best score for each document\n",
    "    for (doc_idx, chunk), score in zip(doc_chunks, chunk_scores):\n",
    "        if doc_idx not in doc_best_scores or score > doc_best_scores[doc_idx]:\n",
    "            doc_best_scores[doc_idx] = score\n",
    "    \n",
    "    # Rank documents by their best chunk score\n",
    "    ranked = sorted(\n",
    "        [(doc_idx, score, docs[doc_idx]) for doc_idx, score in doc_best_scores.items() if score > 0],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    if not ranked:\n",
    "        print(\"[BM25] No documents scored above 0\")\n",
    "        return docs\n",
    "    \n",
    "    # Log ranking results\n",
    "    print(f\"[BM25] Ranked {len(ranked)} documents by heading+content chunks\")\n",
    "    for i, (doc_idx, score, _) in enumerate(ranked[:5], 1):\n",
    "        print(f\"  [{i}] Doc {doc_idx}: score={score:.4f}\")\n",
    "    \n",
    "    # Return sorted documents\n",
    "    ranked_docs = [doc for _, _, doc in ranked]\n",
    "    \n",
    "    # Add unranked docs at the end (docs with score 0)\n",
    "    unranked_indices = set(range(len(docs))) - {idx for idx, _, _ in ranked}\n",
    "    ranked_docs.extend([docs[idx] for idx in unranked_indices])\n",
    "    \n",
    "    return ranked_docs[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ec175",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_results = rank_documents_by_keywords(results, ranking_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b332f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
