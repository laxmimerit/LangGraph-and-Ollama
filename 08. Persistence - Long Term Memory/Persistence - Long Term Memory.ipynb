{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7371bd13",
   "metadata": {},
   "source": [
    "## Long-Term Memory - Remember Across Conversations\n",
    "### Store User Preferences That Work Across All Threads\n",
    "\n",
    "Learning Objectives:\n",
    "- Understand short-term vs long-term memory\n",
    "- Store user preferences across threads automatically\n",
    "- Search memories with semantic search\n",
    "\n",
    "#### Real-World Use Cases:\n",
    "1. **Personal Assistants**: Remember preferences across all chats\n",
    "2. **Customer Support**: Track customer info across tickets\n",
    "3. **E-commerce**: Store shopping preferences\n",
    "4. **Education**: Track learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w5usamwtbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"qwen3\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3534e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Store Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Create embedding function for semantic search\n",
    "embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=BASE_URL)\n",
    "\n",
    "def embed_texts(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"Embed texts for semantic search.\"\"\"\n",
    "    return embeddings.embed_documents(texts)\n",
    "\n",
    "# store = PostgresStore.from_conn_string(\n",
    "#     \"postgresql://neondb_owner:npg_BozYjT3Ulu0w@ep-shiny-bonus-adldxtd4-pooler.c-2.us-east-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require\",\n",
    "#     index={\"embed\": embed_texts, \"dims\": 768}\n",
    "# )\n",
    "\n",
    "# Create PostgreSQL connection manually\n",
    "db_url = \"postgresql://neondb_owner:npg_BozYjT3Ulu0w@ep-shiny-bonus-adldxtd4-pooler.c-2.us-east-1.aws.neon.tech/neondb?sslmode=require\"\n",
    "store_conn = psycopg.connect(db_url, autocommit=True, prepare_threshold=0)\n",
    "\n",
    "# Pass connection directly to PostgresSaver (no context manager needed)\n",
    "store = PostgresStore(store_conn, index={\"embed\": embed_texts, \"dims\": 768})\n",
    "\n",
    "# Setup tables if first time\n",
    "store.setup()  # Uncomment if running for the first time\n",
    "\n",
    "\n",
    "# Create PostgreSQL connection manually\n",
    "# Pass connection directly to PostgresSaver (no context manager needed)\n",
    "checkpointer_conn = psycopg.connect(db_url, autocommit=True, prepare_threshold=0)\n",
    "checkpointer = PostgresSaver(checkpointer_conn)\n",
    "\n",
    "# Setup tables if first time\n",
    "# checkpointer.setup()  # Uncomment if running for the first time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2yjbzpxj46h",
   "metadata": {},
   "source": [
    "## Tool usage - Memory Management Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7714c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food: {'diet': 'vegetarian', 'likes': ['pasta', 'pizza', 'salad']}\n",
      "Work: {'role': 'Data Scientist', 'interests': ['machine learning', 'Python']}\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Demo 1: Store and Retrieve Memories\n",
    "# =============================================================================\n",
    "\n",
    "# Memory Operations: put(), get(), search(), delete()\n",
    "\n",
    "user_id = \"kgptalkie\"\n",
    "namespace = (user_id, \"preferences\")\n",
    "\n",
    "# PUT - Store memories\n",
    "store.put(namespace, \"food\", {\n",
    "    \"diet\": \"vegetarian\",\n",
    "    \"likes\": [\"pasta\", \"pizza\", \"salad\"]\n",
    "})\n",
    "\n",
    "store.put(namespace, \"work\", {\n",
    "    \"role\": \"Data Scientist\",\n",
    "    \"interests\": [\"machine learning\", \"Python\"]\n",
    "})\n",
    "\n",
    "# GET - Retrieve by key\n",
    "food = store.get(namespace, \"food\")\n",
    "print(f\"Food: {food.value}\")\n",
    "\n",
    "work = store.get(namespace, \"work\")\n",
    "print(f\"Work: {work.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cdf579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this with caution as it will delete the data permanently\n",
    "# store.delete(namespace, \"work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd76106c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Item(namespace=['kgptalkie', 'preferences'], key='work', value={'role': 'Data Scientist', 'interests': ['machine learning', 'Python']}, created_at='2025-10-28T18:46:40.598110+00:00', updated_at='2025-10-28T18:46:40.598110+00:00', score=0.5419190476037246)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Demo 2: Semantic Search\n",
    "# =============================================================================\n",
    "\n",
    "# SEARCH - Find relevant memories\n",
    "query = \"What does Laxmi Kant like to eat?\"\n",
    "results = store.search(namespace, query=query, limit=1)\n",
    "results\n",
    "\n",
    "query = \"Where does Laxmi Kant work?\"\n",
    "results = store.search(namespace, query=query, limit=1)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Agent with Automatic Memory Storage\n",
    "#\n",
    "# Agent automatically saves user information to long-term memory\n",
    "\n",
    "# =============================================================================\n",
    "# State Definition\n",
    "# =============================================================================\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"State with user_id for long-term memory.\"\"\"\n",
    "    messages: Annotated[list, operator.add]\n",
    "    user_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Tools for Memory Management\n",
    "# =============================================================================\n",
    "\n",
    "@tool\n",
    "def save_user_memory(user_id: str, category: str, information: dict) -> str:\n",
    "    \"\"\"\n",
    "    Save user preference or information to long-term memory.\n",
    "\n",
    "    Args:\n",
    "        user_id: User identifier\n",
    "        category: Category of information (e.g., 'food', 'work', 'hobbies', 'schedule', 'location')\n",
    "        information: Dictionary containing the information to save\n",
    "    \"\"\"\n",
    "    namespace = (user_id, \"preferences\")\n",
    "\n",
    "    # Save to store\n",
    "    store.put(namespace, category, information)\n",
    "\n",
    "    return f\"Saved {category} preferences\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_user_memory(user_id: str, category: str) -> str:\n",
    "    \"\"\"\n",
    "    Retrieve user preference or information from long-term memory.\n",
    "\n",
    "    Args:\n",
    "        user_id: User identifier\n",
    "        category: Category of information to retrieve (e.g., 'food', 'work', 'hobbies')\n",
    "    \"\"\"\n",
    "    namespace = (user_id, \"preferences\")\n",
    "\n",
    "    # Retrieve from store\n",
    "    item = store.get(namespace, category)\n",
    "\n",
    "    if item:\n",
    "        return f\"{category}: {item.value}\"\n",
    "    else:\n",
    "        return f\"No {category} information found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d05edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Agent Node with Automatic Memory\n",
    "# =============================================================================\n",
    "\n",
    "def agent_node(state: AgentState) -> dict:\n",
    "    \"\"\"Agent that uses long-term memory automatically.\"\"\"\n",
    "\n",
    "    user_id = state.get(\"user_id\", \"unknown\")\n",
    "    namespace = (user_id, \"preferences\")\n",
    "\n",
    "    # Search memories for context\n",
    "    last_message = state[\"messages\"][-1].content\n",
    "    memories = store.search(namespace, query=last_message, limit=3)\n",
    "\n",
    "    # Build context from memories\n",
    "    context_lines = []\n",
    "    for mem in memories:\n",
    "        context_lines.append(f\"- {mem.key}: {mem.value}\")\n",
    "\n",
    "    memory_text = \"\\n\".join(context_lines) if context_lines else \"No preferences stored yet\"\n",
    "\n",
    "    print(f\"USER Memory Context:\\n{memory_text}\\n\")\n",
    "\n",
    "    # Bind tools to LLM\n",
    "    tools = [save_user_memory, get_user_memory]\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # System message with instructions\n",
    "    system_msg = SystemMessage(content=f\"\"\"You are a helpful assistant with long-term memory capabilities.\n",
    "\n",
    "                    User ID: {user_id}\n",
    "                    Current User Memories:\n",
    "                    {memory_text}\n",
    "\n",
    "                    MEMORY TOOLS USAGE:\n",
    "\n",
    "                    1. save_user_memory: Use when user shares NEW information\n",
    "                    - Always pass user_id: \"{user_id}\"\n",
    "                    - Food preferences (diet, likes, dislikes, allergies)\n",
    "                    - Work information (role, company, interests)\n",
    "                    - Hobbies and activities\n",
    "                    - Schedule and availability\n",
    "                    - Location and timezone\n",
    "\n",
    "                    2. get_user_memory: Use when you need to recall specific category\n",
    "                    - Always pass user_id: \"{user_id}\"\n",
    "                    - When answering questions about past preferences\n",
    "                    - When user asks \"what do you know about me?\"\n",
    "                    - When making recommendations based on preferences\n",
    "\n",
    "                    GUIDELINES:\n",
    "                    - Always save when user shares personal information\n",
    "                    - Retrieve specific categories when needed for context\n",
    "                    - Use semantic search results shown above for general context\n",
    "                    - Be conversational and natural when saving/retrieving\"\"\")\n",
    "\n",
    "    messages = [system_msg] + state[\"messages\"]\n",
    "\n",
    "    # Generate response\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        for tc in response.tool_calls:\n",
    "            print(f\"[AGENT] called Tool {tc.get('name', '?')} with args {tc.get('args', '?')}\")\n",
    "    else:\n",
    "        print(f\"[AGENT] Responding...\")\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e84e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routing\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Route to tools or end.\"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    if hasattr(last, \"tool_calls\") and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51123471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Tool Node\n",
    "# =============================================================================\n",
    "\n",
    "# Use ToolNode directly - tools handle store operations themselves\n",
    "from langgraph.prebuilt import ToolNode\n",
    "tool_node = ToolNode([save_user_memory, get_user_memory])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3wi0xw5o9xj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chat Helper Function\n",
    "# =============================================================================\n",
    "\n",
    "def chat(agent, query, user_id, thread_id):\n",
    "    \"\"\"Helper function for streaming chat with memory.\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    for chunk in agent.stream({'messages': [query], 'user_id': user_id}, config=config):\n",
    "\n",
    "        if 'agent' in chunk:\n",
    "            chunk_data = chunk.get('agent')\n",
    "        else:\n",
    "            chunk_data = chunk.get('tools')\n",
    "\n",
    "        if chunk_data and 'messages' in chunk_data:\n",
    "            msg = chunk_data['messages'][0]\n",
    "            \n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                for tc in msg.tool_calls:\n",
    "                    print(f\"[AGENT] called Tool {tc.get('name', '?')} with args {tc.get('args', '?')}\")\n",
    "            else:\n",
    "                print(f\"[AGENT/ToolMessage] Responding.\\n{msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d99a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Graph\n",
    "# =============================================================================\n",
    "\n",
    "def create_agent():\n",
    "    \"\"\"Create agent with automatic long-term memory.\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "    builder.add_node(\"agent\", agent_node)\n",
    "    builder.add_node(\"tools\", tool_node)  # ToolNode handles tool execution\n",
    "    \n",
    "    builder.add_edge(START, \"agent\")\n",
    "    builder.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "    \n",
    "    builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cafe6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent()\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5640e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"kgptalkie\"\n",
    "config = {\"configurable\": {\"thread_id\": f\"{user_id}_chat1\"}}\n",
    "\n",
    "# User shares information - agent automatically saves it\n",
    "agent = create_agent()\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"I am Laxmi Kant Tiwari. I love AI and machine learning.\")],\n",
    "    \"user_id\": user_id\n",
    "}, config)\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c979901",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"kgptalkie\"\n",
    "config = {\"configurable\": {\"thread_id\": f\"{user_id}_chat2\"}}\n",
    "\n",
    "# User shares information - agent automatically saves it\n",
    "agent = create_agent()\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"What do you know about me?\")],\n",
    "    \"user_id\": user_id\n",
    "}, config)\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"laxmikant\"\n",
    "config = {\"configurable\": {\"thread_id\": f\"{user_id}_chat1\"}}\n",
    "\n",
    "# User shares information - agent automatically saves it\n",
    "agent = create_agent()\n",
    "result = agent.invoke({\n",
    "    \"messages\": [HumanMessage(\"What do you know about me?\")],\n",
    "    \"user_id\": user_id\n",
    "}, config)\n",
    "\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deedb814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What do you know about me?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I currently don't have any stored information about you. Would you like to share some details about your preferences, work, hobbies, or other interests? That way I can better assist you!\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-10-28T18:49:19.956364Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1040576600, 'load_duration': 65478200, 'prompt_eval_count': 529, 'prompt_eval_duration': 42151500, 'eval_count': 173, 'eval_duration': 886524900, 'model_name': 'qwen3', 'model_provider': 'ollama'}, id='lc_run--b8bca998-6f04-4802-a2a6-9f8bbe77c149-0', usage_metadata={'input_tokens': 529, 'output_tokens': 173, 'total_tokens': 702})],\n",
       " 'user_id': 'laxmikant'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e42a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
