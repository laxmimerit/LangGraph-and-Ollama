{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7dc2879",
   "metadata": {},
   "source": [
    "## Agentic Memory and Streaming\n",
    "### Memory & Streaming - Stateful, Responsive Agents\n",
    "\n",
    "Learning Objectives:\n",
    "- Implement conversation memory with checkpointers\n",
    "- Use thread_id for multiple conversations\n",
    "- Stream responses for real-time UX\n",
    "\n",
    "#### Real-World Use Cases:\n",
    "1. Customer Support: Remember customer issues across sessions\n",
    "2. Personal Assistants: Maintain user preferences and history\n",
    "3. Educational Tutors: Track learning progress\n",
    "4. Creative Tools: Continue stories, designs across sessions\n",
    "5. Code Assistants: Remember project context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:11434\"\n",
    "MODEL_NAME = \"qwen3\"\n",
    "\n",
    "llm = ChatOllama(model=MODEL_NAME, base_url=BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tool usage\n",
    "import sys\n",
    "sys.path.append(r\"../05. LangGraph ReAct Agent with Tools\")\n",
    "\n",
    "import my_tools\n",
    "\n",
    "all_tools = [my_tools.get_weather, my_tools.calculate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f660b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f72302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Nodes\n",
    "# =============================================================================\n",
    "\n",
    "def agent_node(state: AgentState) -> dict:\n",
    "    \"\"\"Agent with tools and memory.\"\"\"\n",
    "    \n",
    "    llm_with_tools = llm.bind_tools(all_tools)\n",
    "\n",
    "    system_message = SystemMessage(content=\"\"\"You are a friendly assistant with memory and access to documentation search.\n",
    "        \"Use the available tools to help the user when necessary.\"\"\")\n",
    "\n",
    "    messages = [system_message] + state['messages']\n",
    "        \n",
    "    # LLM decides whether to use tools or respond directly\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    return {\"messages\": [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Route to tools or end.\"\"\"\n",
    "    last = state[\"messages\"][-1]\n",
    "    # If there are tool calls, route to tools\n",
    "    if hasattr(last, \"tool_calls\") and last.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we're done\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Graph\n",
    "# =============================================================================\n",
    "\n",
    "def create_agent():\n",
    "    \"\"\"Create chatbot with tools, memory, and streaming support.\"\"\"\n",
    "    builder = StateGraph(AgentState)\n",
    "\n",
    "    # Add nodes\n",
    "    builder.add_node(\"agent\", agent_node)\n",
    "    builder.add_node(\"tools\", ToolNode(all_tools))\n",
    "\n",
    "    # Define flow\n",
    "    builder.add_edge(START, \"agent\")\n",
    "    # Conditional routing: tools or end\n",
    "    builder.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n",
    "    # After tools, go back to agent\n",
    "    builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    # Add checkpointer for memory across invocations\n",
    "    checkpointer = MemorySaver()\n",
    "    # Compile with checkpointer enables memory\n",
    "    return builder.compile(checkpointer=checkpointer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f13a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_agent()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke graph with same thread_id to maintain memory\n",
    "msg = \"Hello, My name is Laxmi Kant Tiwari.\"\n",
    "\n",
    "# msg = \"Can you search the documentation for 'langgraph' and summarize it for me?\"\n",
    "# msg = \"Can you search the documentation for 'langchain' and summarize it for me?\"\n",
    "\n",
    "thread_id = \"laxmikant\"\n",
    "\n",
    "\n",
    "def chat(msg, thread_id):\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    for chunk in graph.stream({\"messages\": [msg]}, config):\n",
    "        # print(\"\\n--- New Chunk ---\\n\", chunk)\n",
    "\n",
    "        if 'agent' in chunk:\n",
    "            chunk = chunk.get('agent')\n",
    "        else:\n",
    "            chunk = chunk.get('tools')\n",
    "            \n",
    "        # check chunk is tool message\n",
    "        if hasattr(chunk[\"messages\"][-1], \"tool_calls\") and chunk[\"messages\"][-1].tool_calls:\n",
    "            for tc in chunk[\"messages\"][-1].tool_calls:\n",
    "                # tool name and arguments\n",
    "                print(f\"\\n[TOOL CALL] {tc.get('name')} with args {tc.get('args')}\")\n",
    "        else:\n",
    "            print(f\"\\n[AGENT] {chunk['messages'][-1].content}\")\n",
    "\n",
    "chat(msg, thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"what is my name? and summarize the previous answer.\"\n",
    "thread_id = \"laxmikant\"\n",
    "\n",
    "chat(msg, thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2207cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"what is my name? and summarize the previous answer.\"\n",
    "thread_id = \"kgptalkie\"\n",
    "\n",
    "chat(msg, thread_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = \"what is my name? and summarize the previous answer.\"\n",
    "thread_id = \"laxmikant\"\n",
    "\n",
    "chat(msg, thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e61e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c69713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943beda0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
