{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– LLM and AI Agent Development Courses\n",
    "\n",
    "| Course | Description | Enroll |\n",
    "|--------|-------------|---------|\n",
    "| **ðŸŽ¯ Master OpenAI Agent Builder** | Build and deploy AI agents visually using OpenAI Agent Builder, ChatKit, RAG, Chatbot, AI Assistant with MCP, AWS, RDS MySQL | [Enroll Now](https://www.udemy.com/course/master-openai-agent-builder-low-code-ai-projects-workflow/?referralCode=B0B67D18B1013E488FB7) |\n",
    "| **ðŸ”¥ MCP Mastery** | Build MCP servers & clients with Python, Streamlit, ChromaDB, LangChain, LangGraph agents, and Ollama integrations | [Enroll Now](https://www.udemy.com/course/mcp-mastery-build-ai-apps-with-claude-langchain-and-ollama/?referralCode=31C17C306A59601B8689) |\n",
    "| **ðŸ“Š Agentic RAG with LangChain** | Step-by-Step Guide to RAG with LangChain v1, LangGraph, and Ollama (Qwen3, Gemma3, DeepSeek-R1, LLAMA, FAISS) | [Enroll Now](https://www.udemy.com/course/agentic-rag-with-langchain-and-langgraph/?referralCode=C0BCC208F53AF2C98AC5) |\n",
    "| **ðŸ”§ Master LangGraph and LangChain** | Agentic RAG and Chatbot, AI Agent with LangChain v1, Qwen3, Gemma3, DeepSeek-R1, LLAMA 3.2, FAISS Vector Database | [Enroll Now](https://www.udemy.com/course/langgraph-with-ollama/?referralCode=B646DCB44A189BEBC20C) |\n",
    "| **âš¡ Master Langchain and Ollama** | Master Langchain v1, Local LLM Projects with Ollama, Qwen3, Gemma3, DeepSeek-R1, LLAMA 3.2, Complete Integration Guide | [Enroll Now](https://www.udemy.com/course/ollama-and-langchain/?referralCode=7F4C0C7B8CF223BA9327) |\n",
    "| **ðŸ”¬ Fine Tuning LLM** | Learn transformer architecture fundamentals and fine-tune LLMs with custom datasets | [Enroll Now](https://www.udemy.com/course/fine-tuning-llm-with-hugging-face-transformers/?referralCode=6DEB3BE17C2644422D8E) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langchain Getting Started\n",
    "    - Installation\n",
    "    - Langsmith and Env setup\n",
    "    - Environment Variables\n",
    "    - Ollama Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you first pull the model using the ollama CLI\n",
    "# ollama pull gemma3\n",
    "# ollama pull <ollama_model_name> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Messages - The Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model='gemma3',\n",
    "                 base_url=\"http://localhost:11434\",\n",
    "                 temperature=0,\n",
    "                 num_ctx=2048,\n",
    "                 num_predict=512)\n",
    "\n",
    "# Three types of messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"What is 2+2?\"),\n",
    "]\n",
    "\n",
    "# Get response (returns AIMessage)\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)  # \"4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prompt Templates - Reusable Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create template with variables\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert in {subject}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Format with values\n",
    "messages = prompt.invoke({\"subject\": \"math\", \"question\": \"What is 2+2?\"})\n",
    "\n",
    "# Use with LLM\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LCEL - Chains with Pipe Operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Build chain with |\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a fact about {topic}\")\n",
    "\n",
    "# Chain them together\n",
    "chain = prompt | llm\n",
    "\n",
    "# Run chain\n",
    "response = chain.invoke({\"topic\": \"earth\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding Output Parsers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"List 3 colors\")\n",
    "parser = StrOutputParser()  # Extracts string content\n",
    "\n",
    "# Full chain\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({})\n",
    "print(result)  # Plain string, not AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Streaming Responses and Structured Output (Pydantic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define structure\n",
    "class Sentiment(BaseModel):\n",
    "    sentiment: bool = Field(description=\"True if positive sentiment, False otherwise\")\n",
    "    reasoning: str = Field(description=\"Explanation for the sentiment\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(Sentiment)\n",
    "\n",
    "structured_llm.invoke(\"I love programming!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in structured_llm.stream(\"I love programming!\"):\n",
    "    print(chunk, end=\"\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
